#!/usr/bin/env python
# coding: utf-8

from pyclausie import ClausIE
import spacy
from rdflib import Graph
from rdflib import URIRef, BNode, Literal
from rdflib.namespace import RDF, FOAF
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag


cl = ClausIE.get_instance()
'''
Reading the Obama Abstract File
'''
f=open('Obama_abstract.txt')
content=f.read()
nlp=spacy.load('en')
doc=nlp(content)

'''
Parsing the JSON generated by Spotlight
'''

import json

resource_dict={}
with open('Obama_Json.json') as f:
    data = json.load(f)
    for elem in (data['Resources']):
        resource_dict[elem['@surfaceForm'].lower()]= elem['@URI']
        
        
sents=[]

for sent in doc.sents:
    sents.append(sent.text)
    
'''
Using PyClausIE Algorithm to Extract out the Triples in TEXT Format
'''

triples = cl.extract_triples(sents)
lemmatizer = WordNetLemmatizer()

"""
Using a handful of white-list of lexicalized verbs
"""
nlp=spacy.load('en')

import json
with open('type_dict.json') as f:
    type_dict = json.load(f)

with open('white_dict.json') as f:
    white_dict = json.load(f)

"""
Creating the RDF Graph using the triples
"""

rdfGraph = Graph()

for triple in triples:
    sentence = str(triple).split(",")[1].split("=")[1][1:-1]+ " "+str(str(triple).split(",")[2].split("=")[1])[1:-1]+" "+str(str(triple).split(",")[3].split("=")[1])[1:-2]
    
    s=str(triple).split(",")[1].split("=")[1][1:-1].lower()
    p=str(str(triple).split(",")[2].split("=")[1])[1:-1].lower()
    o=str(str(triple).split(",")[3].split("=")[1])[1:-2].lower()
    
    """
    Post-processing the triples
    """
    modified_p=lemmatizer.lemmatize(p, 'v')+" "+str(o.split(" ")[0])
    if(modified_p in white_dict.keys() or modified_p in type_dict.keys()):
        p=modified_p
        o=" ".join(o.split(" ")[1:])
    
       
    
    """
    subject checking 
    """
    nps=[]
    doc=nlp(s)
    for np in doc.noun_chunks:
        nps.append(np.text)
        
      
    '''
    Substituting the DBPedia Spotlight Resource URLS
    '''
    subjects=[]
    if(len(nps)>0):
        if(nps[0].lower() in resource_dict.keys()):
            subjects.append(resource_dict[nps[0]])
            subject = URIRef(subjects[0])
        else:
            for key, elem in resource_dict.items():
                if(key.lower() in nps[0].lower()):
                    subjects.append(nps[0].replace(key,resource_dict[key]))
                    
    else:
        subjects.append(s)
         
    """
    Predicate checking 
    """
    
    matched=[]
    for x in resource_dict.keys():
        if(len(matched)>0):
            for elem in matched:
                if(x in p.lower() and x not in elem):
                    matched.append(x)
        elif(len(matched)==0):
            if(x in p.lower()):
                matched.append(x)
    p=p.lower()
    for elem in list(set(matched)):
        p=p.replace(elem,resource_dict[elem])
    
    #f
    predicate = BNode()
    #if(p == 'be a' or p == 'be an'  or p == 'be the' ):
    #print(p)
    if(p in type_dict.keys()):
        rdfGraph.add((predicate, RDF.type, Literal(RDF.type)))
    elif(p in white_dict.keys()):
        rdfGraph.add((predicate,  FOAF.name, Literal(white_dict[p])))
    else:
        rdfGraph.add((predicate, FOAF.name, Literal(p)))
    
    """
    Object checking 
    """
    
    
    matched=[]
    for x in resource_dict.keys():
        if(len(matched)>0):
            for elem in matched:
                if(x in o.lower() and x not in elem):
                    matched.append(x)
        elif(len(matched)==0):
            if(x in o.lower()):
                matched.append(x)
    o=o.lower()
    '''
    Substituting the DBPedia Spotlight Resource URLS
    '''
    
    for elem in list(set(matched)):
        o=o.replace(elem,resource_dict[elem])
    
    for key, value in white_dict.items():
        if(key in o):
            #print(key, o)
            o=o.replace(key,white_dict[key])
            #print(o, white_dict[key])
    obj = BNode()
    rdfGraph.add((obj, FOAF.name, Literal(o)))
    
    rdfGraph.add((subject, predicate, obj))


print(rdfGraph.serialize(format='ttl').decode('utf-8'))




